# SemanticLayer: Data Aggregation & Analytics Platform

[![GitHub Actions](https://github.com/AdarshInturi0425/AI-Project/workflows/SemanticLayer%20Tests/badge.svg)](https://github.com/AdarshInturi0425/AI-Project/actions)
[![Python 3.8+](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A production-ready data pipeline that transforms raw transaction data into semantic analytics layer using **PySpark** (with pandas fallback) and **DuckDB** for SQL queries.

---

## ‚ö° Quick Start

### One-Command Setup (recommended)

```bash
# macOS/Linux
chmod +x quick_start.sh
./quick_start.sh

# Windows PowerShell
powershell -ExecutionPolicy Bypass -File quick_start.ps1
```

This will:
1. ‚úÖ Setup Python environment
2. ‚úÖ Install dependencies
3. ‚úÖ Run ETL pipeline
4. ‚úÖ Display summary statistics
5. ‚úÖ Run all tests

### Manual Setup

```bash
# Clone repo
git clone https://github.com/AdarshInturi0425/AI-Project.git
cd AI-Project

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # macOS/Linux
# OR
.venv\Scripts\Activate.ps1  # Windows PowerShell

# Install dependencies
pip install -r SemanticLayer/requirements.txt

# Run ETL
python SemanticLayer/scripts/process_data_spark.py

# View results
head -20 SemanticLayer/data/gold/gold_view.csv

# Run tests
pytest -q
```

---

## üìã Features

### Data Pipeline (PySpark + Pandas)
- **Raw Layer**: Input CSV files with customer and transaction data
- **Silver Layer**: Cleaned, deduplicated data with type validation
- **Gold Layer**: Aggregated semantic views for analytics
- **Metadata**: Schema tracking and data lineage

### Query Layer (DuckDB)
- In-memory SQL queries on gold layer
- High-performance analytics without heavy infrastructure
- Export results as pandas DataFrames or CSV

### Validation & Testing
- Automated data quality checks
- Unit tests for ETL aggregations
- Integration tests with pytest
- CI/CD via GitHub Actions

### Documentation
- OS-specific setup guides (Windows, macOS, Linux)
- Comprehensive troubleshooting notebook
- 10+ example SQL queries
- Contributing guidelines

---

## üìÅ Project Structure

```
SemanticLayer/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                    # Input data
‚îÇ   ‚îú‚îÄ‚îÄ silver/                 # Cleaned data
‚îÇ   ‚îú‚îÄ‚îÄ gold/                   # Semantic analytics layer
‚îÇ   ‚îî‚îÄ‚îÄ metadata.json           # Schema metadata
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ process_data_spark.py   # PySpark ETL (recommended)
‚îÇ   ‚îú‚îÄ‚îÄ process_data.py         # Pandas ETL (fallback)
‚îÇ   ‚îú‚îÄ‚îÄ sql_layer.py            # DuckDB queries
‚îÇ   ‚îú‚îÄ‚îÄ summary_stats.py        # Summary statistics
‚îÇ   ‚îî‚îÄ‚îÄ data_validation.py      # Data quality checks
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ semantic_layer_demo_colab.ipynb    # PySpark demo
‚îÇ   ‚îú‚îÄ‚îÄ colab_duckdb.ipynb                 # DuckDB demo (no Java)
‚îÇ   ‚îî‚îÄ‚îÄ TROUBLESHOOTING.ipynb              # Troubleshooting guide
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_etl.py             # Pytest tests
‚îú‚îÄ‚îÄ requirements.txt            # Dependencies
‚îî‚îÄ‚îÄ README.md                   # This file
```

---

## üöÄ Usage Examples

### 1. Generate Summary Statistics

```bash
python SemanticLayer/scripts/summary_stats.py
```

**Output:**
```
Total Customers:        520
Total Spend:           $48,500.00
Total Transactions:     1,250
Avg Transaction Amount: $38.80
```

### 2. Run SQL Queries

```bash
python SemanticLayer/scripts/sql_layer.py
```

Includes:
- Top 5 spenders
- Transaction frequency distribution
- Spending by segment

### 3. Validate Data Quality

```bash
python SemanticLayer/scripts/data_validation.py
```

Checks:
- ‚úì File existence and structure
- ‚úì Null value handling
- ‚úì Numeric ranges
- ‚úì Gold aggregation accuracy

### 4. Run Automated Tests

```bash
pytest -v SemanticLayer/tests/
```

### 5. Query Gold Layer (Python)

```python
import duckdb
import pandas as pd

# Load gold view
conn = duckdb.connect(':memory:')
conn.execute("CREATE TABLE gold_view AS SELECT * FROM 'SemanticLayer/data/gold/gold_view.csv'")

# Top 5 customers
result = conn.execute("""
    SELECT customer_id, total_spend
    FROM gold_view
    ORDER BY total_spend DESC
    LIMIT 5
""").fetch_df()

print(result)
```

---

## üìä Example Queries

See [EXAMPLE_QUERIES.md](SemanticLayer/EXAMPLE_QUERIES.md) for:

1. Top customers by spend
2. Most frequent customers
3. Spending distribution (quartiles)
4. High-value customers (90th percentile)
5. Customer segmentation
6. Average transaction value analysis
7. Total business metrics
8. Customer filtering
9. Outlier detection
10. Time-based analysis (template)

---

## üîß Configuration

### Adding Custom Data

1. Place raw CSV files in `SemanticLayer/data/raw/`
2. Update schema in relevant script
3. Run ETL: `python SemanticLayer/scripts/process_data_spark.py`

### Customizing Aggregations

Edit `SemanticLayer/scripts/process_data_spark.py`:

```python
# Modify gold layer aggregations
gold_df = silver_transactions.groupBy("customer_id").agg(
    F.sum("amount").alias("total_spend"),
    F.count("transaction_id").alias("transaction_count"),
    F.avg("amount").alias("avg_transaction_amount"),
    # Add custom metrics here
).collect()
```

---

## üìö Documentation

| Document | Purpose |
|----------|---------|
| [SETUP_GUIDE.md](SETUP_GUIDE.md) | Complete setup instructions |
| [SETUP_BY_OS.md](SETUP_BY_OS.md) | OS-specific guides (Win/Mac/Linux) |
| [TROUBLESHOOTING.ipynb](SemanticLayer/notebooks/TROUBLESHOOTING.ipynb) | Common issues & solutions |
| [EXAMPLE_QUERIES.md](SemanticLayer/EXAMPLE_QUERIES.md) | 10+ SQL query examples |
| [CONTRIBUTING.md](CONTRIBUTING.md) | Contribution guidelines |

---

## üêç Python Versions & Dependencies

**Supported Python:** 3.8, 3.9, 3.10, 3.11

**Key Dependencies:**
- `pyspark>=3.4.0` ‚Äì Data processing
- `pandas>=1.3.0` ‚Äì Data manipulation
- `duckdb>=0.8.0` ‚Äì SQL queries
- `pytest>=7.0.0` ‚Äì Testing

See `SemanticLayer/requirements.txt` for full list.

---

## ‚ö†Ô∏è Prerequisites

### Required
- **Python 3.8+** ([download](https://www.python.org/downloads/))
- **Git** ([download](https://git-scm.com/))

### For PySpark ETL
- **Java/JDK 11+** ([download](https://adoptium.net/))

### Optional
- **Docker** (for containerized execution)
- **Google Colab** (for notebook demos)

---

## üèÉ Troubleshooting

### Common Issues

| Issue | Solution |
|-------|----------|
| `Java not found` | Install JDK 11+, set `JAVA_HOME` env var |
| `Module not found` | Activate venv: `source .venv/bin/activate` |
| `gold_view.csv missing` | Run ETL: `python SemanticLayer/scripts/process_data_spark.py` |
| `Tests fail` | Re-generate data, check file paths, see `TROUBLESHOOTING.ipynb` |

**Full troubleshooting guide:** See [TROUBLESHOOTING.ipynb](SemanticLayer/notebooks/TROUBLESHOOTING.ipynb)

---

## üß™ Testing

```bash
# Run all tests
pytest -v

# Run specific test file
pytest -v SemanticLayer/tests/test_etl.py

# Run with coverage report
pytest --cov=SemanticLayer SemanticLayer/tests/

# Run tests matching pattern
pytest -k "gold_view" -v
```

---

## ü§ù Contributing

We welcome contributions! Please read [CONTRIBUTING.md](CONTRIBUTING.md) for:
- Branching strategy
- Code style guidelines
- Testing requirements
- Pull request process

Quick contribution steps:
```bash
git checkout -b feature/my-feature
# Make changes
pytest -v  # Verify tests pass
git commit -m "[FEATURE] Add my feature"
git push origin feature/my-feature
# Open Pull Request on GitHub
```

---

## üìà Performance

Typical runtime on sample data (520 customers, 1,250 transactions):
- **PySpark ETL**: 15-30 seconds (includes JVM startup)
- **SQL queries**: <1 second (DuckDB)
- **Data validation**: 2-5 seconds
- **Tests**: 20-40 seconds

---

## üê≥ Docker Support

Run in Docker to avoid system dependencies:

```bash
docker build -t semantic-layer .
docker run semantic-layer
```

See `Dockerfile` for details.

---

## üìù License

This project is licensed under the **MIT License** - see [LICENSE](LICENSE) file for details.

---

## üë• Authors

- **Adarsh Inturi** ([@AdarshInturi0425](https://github.com/AdarshInturi0425))

---

## üôè Acknowledgments

- PySpark & Spark community
- DuckDB for in-memory SQL engine
- pytest for testing framework
- Pandas for data manipulation

---

## üí° Next Steps

1. ‚úÖ [Setup](SETUP_GUIDE.md) the project
2. ‚úÖ [Explore](EXAMPLE_QUERIES.md) example queries
3. ‚úÖ [Run tests](README.md#-testing) to verify installation
4. ‚úÖ [Contribute](CONTRIBUTING.md) improvements
5. ‚úÖ Deploy to production ([guide coming soon](docs/DEPLOYMENT.md))

---

## üìû Support

- **Issues**: [GitHub Issues](https://github.com/AdarshInturi0425/AI-Project/issues)
- **Discussions**: [GitHub Discussions](https://github.com/AdarshInturi0425/AI-Project/discussions)
- **Email**: contact@example.com

---

**Made with ‚ù§Ô∏è by the AI-Project team**